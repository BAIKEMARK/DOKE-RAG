# -*- coding: utf-8 -*-

"""
å…¨è‡ªåŠ¨æ‰¹é‡å®éªŒè¿è¡Œè„šæœ¬
========================

åŠŸèƒ½:
1. å¯¹é¢„å®šä¹‰çš„å‚æ•°ç½‘æ ¼ (cosine_threshold, top_k) è¿›è¡Œéå†ã€‚
2. å¯¹æ¯ä¸€ç»„ç½‘æ ¼å‚æ•°ï¼Œè¿è¡Œä¸€ç»„æ ¸å¿ƒé…ç½® (Core Configurations)ã€‚
3. é€šè¿‡å‘½ä»¤è¡Œå‚æ•° `--group` é€‰æ‹©è¦è¿è¡Œçš„æ ¸å¿ƒé…ç½®ç»„ ('group1' æˆ– 'group2')ã€‚
4. è‡ªåŠ¨åˆ›å»ºç»“æ„åŒ–çš„è¾“å‡ºç›®å½•æ¥ä¿å­˜æ¯æ¬¡å®éªŒçš„ç»“æœã€‚
5. ä¸²è¡Œæ‰§è¡Œï¼Œç¡®ä¿æ¯æ¬¡å®éªŒå®Œå…¨ç‹¬ç«‹ï¼Œé¿å…æ•°æ®æ··æ·†ã€‚
6. æ™ºèƒ½æ–­ç‚¹ç»­è·‘ï¼šè‡ªåŠ¨è·³è¿‡å·²æœ‰æœ‰æ•ˆç»“æœçš„å®éªŒï¼Œå¹¶é‡æ–°è¿è¡Œå¤±è´¥çš„æˆ–ç»“æœä¸ºç©ºçš„å®éªŒã€‚
7. è·¯å¾„è‡ªé€‚åº”ï¼šè‡ªåŠ¨å¯»æ‰¾ä¸è„šæœ¬åœ¨åŒä¸€ç›®å½•ä¸‹çš„é—®é¢˜æ–‡ä»¶ã€‚

å¦‚ä½•ä½¿ç”¨:
1. å°†æ­¤æ–‡ä»¶ä¿å­˜ä¸º `run_batch_experiment.py`ã€‚
2. ç¡®ä¿ä½ çš„ç¯å¢ƒä¸­å·²å®‰è£… `lightrag` åŠå…¶ä¾èµ–ã€‚
3. å°†é—®é¢˜æ–‡ä»¶ (ä¾‹å¦‚ `questions_updated.json`) ä¸æ­¤è„šæœ¬æ”¾åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹ã€‚
4. è®¾ç½®å¥½è¿è¡Œ `group1` æ‰€éœ€çš„ç¯å¢ƒå˜é‡ã€‚
5. æ‰“å¼€ç»ˆç«¯ï¼Œè¿è¡Œå‘½ä»¤:
   python experiments/run_batch_evaluation.py --group group1 --base_output_dir "./evaluation/results/batch_experiment"

6. è®¾ç½®å¥½è¿è¡Œ `group2` æ‰€éœ€çš„ç¯å¢ƒå˜é‡ã€‚
7. åœ¨ç»ˆç«¯ä¸­ï¼Œè¿è¡Œå‘½ä»¤:
   python experiments/run_batch_evaluation.py --group group2 --base_output_dir "./evaluation/results/batch_experiment"
"""

import os
import json
import argparse
import itertools
import traceback
from pathlib import Path
from doke_rag.core import LightRAG, QueryParam
from doke_rag.core.llm.openai import openai_complete_if_cache
from doke_rag.core.llm.ollama import ollama_embed
from doke_rag.core.utils import EmbeddingFunc, always_get_an_event_loop, TokenTracker
from doke_rag.config.paths import WORKING_DIR, RESULTS_DIR, ensure_dir

# ==============================================================================
# 1. åœ¨è¿™é‡Œå®šä¹‰ä½ çš„æ‰€æœ‰å®éªŒå‚æ•°
# ==============================================================================

# å‚æ•°ç½‘æ ¼
GRID_PARAMS = {
    "cosine_threshold": [0.2, 0.4, 0.6, 0.8],
    "top_k": [20, 40, 60, 80],
}

# æ ¸å¿ƒé…ç½®ç»„: åˆ†æˆä¸¤ç»„ï¼Œé€šè¿‡å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©
# æ³¨æ„ï¼šworking_dir åº”ä½¿ç”¨ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
# å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ WORKING_DIR è®¾ç½®åŸºç¡€ç›®å½•ï¼Œæˆ–ç›´æ¥ä½¿ç”¨ç›¸å¯¹è·¯å¾„
CONFIG_GROUPS = {
    "group1": [
        # ç¤ºä¾‹é…ç½®ï¼šå–æ¶ˆæ³¨é‡Šå¹¶æ ¹æ®éœ€è¦ä¿®æ”¹è·¯å¾„
        # {
        #     "mode": "hybrid",
        #     "working_dir": "./data/run_data/experiment1/merged_no_textbook",
        #     "f_name": "stru_mech",
        # },
        # {
        #     "mode": "hybrid",
        #     "working_dir": "./data/run_data/experiment1/unmerged_no_textbook",
        #     "f_name": "_manual",
        # },
        {
            "mode": "hybrid",
            "working_dir": "./data/run_data/experiment1/unsplited_no_textbook",
            "f_name": "_split",
        },
    ],
    "group2": [
        {
            "mode": "hybrid",
            "working_dir": "./data/run_data/experiment2/lightrag_baseline",
            "f_name": "lightrag",
        },
        {
            "mode": "naive",
            "working_dir": "./data/run_data/experiment2/lightrag_baseline",
            "f_name": "naive",
        },
        {
            "mode": "hybrid",
            "working_dir": "./data/run_data/experiment2/manual_only",
            "f_name": "only_manual",
        },
    ],
}

# ==============================================================================
# 2. è¾…åŠ©å‡½æ•°å’Œå…¨å±€å¯¹è±¡
# ==============================================================================

token_tracker = TokenTracker()


async def llm_model_func_aliyun(prompt, **kwargs) -> str:
    # ç¡®ä¿ API Key ä»ç¯å¢ƒå˜é‡ä¸­æ­£ç¡®è¯»å–
    api_key = os.getenv("ALIYUN_API_KEY")
    if not api_key:
        raise ValueError("ALIYUN_API_KEY environment variable not set.")
    return await openai_complete_if_cache(
        "deepseek-r1-distill-qwen-14b",
        prompt,
        api_key=api_key,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        token_tracker=token_tracker,
        **kwargs,
    )


def extract_queries_from_json(file_path: str) -> list[str]:
    """ä» JSON æ–‡ä»¶ä¸­å®‰å…¨åœ°è¯»å–é—®é¢˜åˆ—è¡¨ã€‚"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data.get("questions", [])
    except FileNotFoundError:
        print(f"Error: Questions file not found at {file_path}")
        return []
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from {file_path}")
        return []


async def process_query(
    query_text: str, rag_instance: LightRAG, query_param: QueryParam
) -> tuple[dict | None, dict | None]:
    """å¤„ç†å•ä¸ªæŸ¥è¯¢ï¼Œè¿”å›ç»“æœå’Œé”™è¯¯ä¿¡æ¯ã€‚"""
    token_tracker.reset()
    try:
        result = await rag_instance.aquery(query_text, param=query_param)
        usage = token_tracker.get_usage()
        return {
            "query": query_text,
            "result": result,
            "input_tokens": usage["prompt_tokens"],
            "output_tokens": usage["completion_tokens"],
        }, None
    except Exception as e:
        return None, {"query": query_text, "error": str(e)}


def run_queries_and_save_to_json(
    queries: list[str],
    rag_instance: LightRAG,
    query_param: QueryParam,
    output_file: str,
    error_file: str,
):
    """åœ¨ä¸€ä¸ªäº‹ä»¶å¾ªç¯ä¸­è¿è¡Œæ‰€æœ‰æŸ¥è¯¢å¹¶ä¿å­˜ç»“æœã€‚"""
    loop = always_get_an_event_loop()
    with (
        open(output_file, "w", encoding="utf-8") as result_file,
        open(error_file, "w", encoding="utf-8") as err_file,
    ):
        result_file.write("[\n")
        first_entry = True
        error_entries = []

        for i, query_text in enumerate(queries):
            print(f"    - Processing query {i + 1}/{len(queries)}...")
            result, error = loop.run_until_complete(
                process_query(query_text, rag_instance, query_param)
            )
            if result:
                if not first_entry:
                    result_file.write(",\n")
                json.dump(result, result_file, ensure_ascii=False, indent=4)
                first_entry = False
            elif error:
                error_entries.append(error)

        if error_entries:
            json.dump(error_entries, err_file, ensure_ascii=False, indent=4)

        result_file.write("\n]")


# ==============================================================================
# 3. å•æ¬¡å®éªŒçš„æ ¸å¿ƒé€»è¾‘
# ==============================================================================


def run_single_experiment(config: dict):
    """
    è¿è¡Œå•æ¬¡å®éªŒçš„æ ¸å¿ƒå‡½æ•°ã€‚
    'config' å­—å…¸åŒ…å«äº†è¿è¡Œä¸€æ¬¡æ‰€éœ€çš„æ‰€æœ‰å‚æ•°ã€‚
    """
    print(
        f"--- Initializing experiment: {config['f_name']} (cs={config['cosine_threshold']}, tk={config['top_k']}) ---"
    )
    print(f"    Working directory: {config['working_dir']}")

    # 1. åˆå§‹åŒ– LightRAG å®ä¾‹
    rag = LightRAG(
        working_dir=config["working_dir"],
        llm_model_func=llm_model_func_aliyun,
        embedding_func=EmbeddingFunc(
            embedding_dim=768,
            max_token_size=8192,
            func=lambda texts: ollama_embed(
                texts,
                embed_model="nomic-embed-text:latest",
                host="http://localhost:11434",
            ),
        ),
        vector_db_storage_cls_kwargs={
            "cosine_better_than_threshold": config["cosine_threshold"]
        },
        llm_model_max_async=4,
        llm_model_max_token_size=32768,
        entity_extract_max_gleaning=1,
        entity_summary_to_max_tokens=500,
        addon_params={"example_number": 3, "insert_batch_size": 50},
        chunk_token_size=1200,
        chunk_overlap_token_size=100,
        enable_llm_cache=False,
    )

    # 2. è®¾ç½®æŸ¥è¯¢å‚æ•°
    query_param = QueryParam(
        mode=config["mode"],
        top_k=config["top_k"],
        max_token_for_text_unit=4000,
        max_token_for_global_context=4000,
        max_token_for_local_context=4000,
    )

    # 3. å‡†å¤‡è¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_dir = config["output_dir"]
    os.makedirs(output_dir, exist_ok=True)
    output_file_path = os.path.join(output_dir, f"{config['f_name']}_result.json")
    error_file_path = os.path.join(output_dir, f"{config['f_name']}_errors.json")

    # 4. æ‰§è¡ŒæŸ¥è¯¢å¹¶ä¿å­˜ç»“æœ
    print(f"    Starting queries...")
    run_queries_and_save_to_json(
        config["queries"], rag, query_param, output_file_path, error_file_path
    )
    print(f"--- âœ… Finished experiment. Results saved in: {output_dir} ---")


# ==============================================================================
# 4. æ‰¹é‡å®éªŒè¿è¡Œå™¨
# ==============================================================================

def is_result_file_valid(filepath: str) -> bool:
    """
    æ£€æŸ¥ç»“æœæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¹¶ä¸”æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„ã€éç©ºçš„å†…å®¹ã€‚
    """
    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¤§å°å¤§äºä¸€ä¸ªå¾ˆå°çš„å€¼ (ç©ºçš„ "[]" å¤§çº¦æ˜¯2-4å­—èŠ‚)
    if not os.path.exists(filepath) or os.path.getsize(filepath) < 5:
        return False

    # 2. è¯»å–å¹¶å°è¯•è§£æJSONï¼Œç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªéç©ºåˆ—è¡¨
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # å¿…é¡»æ˜¯åˆ—è¡¨ä¸”é•¿åº¦å¤§äº0
        if isinstance(data, list) and len(data) > 0:
            return True
    except (json.JSONDecodeError, IOError):
        # å¦‚æœæ–‡ä»¶æŸåæˆ–æ— æ³•è¯»å–ï¼Œè§†ä¸ºæ— æ•ˆ
        return False

    return False

def main(args):
    """æ‰¹é‡å®éªŒè¿è¡Œå™¨çš„å…¥å£å‡½æ•°ã€‚"""
    # å¦‚æœ questions_file æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œåˆ™å°†å…¶è§£æä¸ºç›¸å¯¹äºè„šæœ¬æ‰€åœ¨ç›®å½•çš„è·¯å¾„
    if not os.path.isabs(args.questions_file):
        script_dir = os.path.dirname(os.path.abspath(__file__))
        args.questions_file = os.path.join(script_dir, args.questions_file)

    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©è¦è¿è¡Œçš„é…ç½®ç»„
    selected_group = CONFIG_GROUPS.get(args.group)
    if not selected_group:
        print(
            f"Error: Invalid group '{args.group}'. Please choose from {list(CONFIG_GROUPS.keys())}"
        )
        return

    # ç”Ÿæˆæ‰€æœ‰ grid å‚æ•°ç»„åˆ
    keys, values = zip(*GRID_PARAMS.items())
    grid_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]

    total_runs = len(grid_combinations) * len(selected_group)
    run_counter = 0

    # ä¸€æ¬¡æ€§è¯»å–é—®é¢˜æ–‡ä»¶ï¼Œä¾›æ‰€æœ‰å®éªŒä½¿ç”¨
    queries = extract_queries_from_json(args.questions_file)
    if not queries:
        print("No queries found. Exiting.")
        return

    print(f"Loaded {len(queries)} questions from '{args.questions_file}'")
    print(
        f"Starting batch for '{args.group}' with a total of {total_runs} experiments..."
    )

    # å¤–å±‚å¾ªç¯ï¼šéå†å‚æ•°ç½‘æ ¼ (e.g., cs=0.2, tk=20)
    for grid_combo in grid_combinations:
        cs = grid_combo["cosine_threshold"]
        tk = grid_combo["top_k"]

        # ä¸ºå½“å‰ç½‘æ ¼ç»„åˆåˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºæ–‡ä»¶å¤¹
        current_output_dir = os.path.join(args.base_output_dir, f"cs{cs}_tk{tk}")

        # å†…å±‚å¾ªç¯ï¼šéå†é€‰æ‹©çš„æ ¸å¿ƒé…ç½®ç»„ (e.g., stru_mech, _manual)
        for core_config in selected_group:
            run_counter += 1
            print(
                f"\n====================== [ Checking {run_counter} / {total_runs} ] ======================"
            )

            # ä½¿ç”¨æ›´æ™ºèƒ½çš„æ–­ç‚¹ç»­è·‘æ£€æŸ¥
            expected_output_file = os.path.join(current_output_dir, f"{core_config['f_name']}_result.json")

            if is_result_file_valid(expected_output_file):
                print(f"âœ… Skipping: Valid result for '{core_config['f_name']}' (cs={cs}, tk={tk}) already exists.")
                continue
            else:
                 print(f"ğŸƒâ€â™‚ï¸ Running: Result for '{core_config['f_name']}' (cs={cs}, tk={tk}) is missing or invalid.")

            # ç»„åˆæˆä¸€ä¸ªå®Œæ•´çš„é…ç½®å­—å…¸
            full_config = {
                **grid_combo,
                **core_config,
                "output_dir": current_output_dir,
                "queries": queries,
            }

            try:
                # è¿è¡Œå•æ¬¡å®éªŒ
                run_single_experiment(full_config)
            except Exception:
                print(f"!!!!!!!!!!!!!! FATAL ERROR IN EXPERIMENT !!!!!!!!!!!!!!")
                print(f"Config that failed: {core_config['f_name']}, cs={cs}, tk={tk}")
                # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
                traceback.print_exc()
                print(f"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
                print("Skipping to the next experiment...")
                continue

    print(
        f"\nğŸ‰ All {total_runs} experiments for group '{args.group}' have been completed."
    )


# ==============================================================================
# 5. è„šæœ¬å…¥å£
# ==============================================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run a batch of LightRAG experiments from a predefined grid search.",
        formatter_class=argparse.RawTextHelpFormatter,
    )

    parser.add_argument(
        "--group",
        type=str,
        required=True,
        choices=["group1", "group2"],
        help="Specify which configuration group to run ('group1' or 'group2').",
    )

    parser.add_argument(
        "--questions_file",
        type=str,
        default="questions_updated.json",
        help="Path to the JSON file containing questions. \nDefaults to a file with this name in the script's directory.",
    )

    parser.add_argument('--base_output_dir', type=str, required=True,
                        help="The base directory where all result folders will be created.\n"
                             "Example: D:\\RAG_Results")

    args = parser.parse_args()
    main(args)