# -*- coding: utf-8 -*-
import os
import json
import asyncio
import re
from pathlib import Path
import random
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv
from tqdm.asyncio import tqdm_asyncio
from tqdm import tqdm
from doke_rag.config.paths import ENV_FILE, RESULTS_DIR

# --- 1. é…ç½®åŒº (è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹) ---

# ã€é…ç½®ã€‘APIå¯†é’¥æ–‡ä»¶è·¯å¾„ - ä»é…ç½®æ¨¡å—å¯¼å…¥
ENV_FILE_PATH = ENV_FILE

# ã€é…ç½®ã€‘è¾“å‡ºç›®å½•
OUTPUT_DIRECTORY = RESULTS_DIR / "Final_PK_Comparison_Report_5Runs"

# ã€é…ç½®ã€‘å¹¶å‘è¯·æ±‚æ•°é‡
CONCURRENT_REQUESTS = 30

# ã€æ–°å¢é…ç½®ã€‘å®éªŒè¿è¡Œæ¬¡æ•°
NUM_EXPERIMENT_RUNS = 5

# ä¸»è§’ A: DOKE-RAG (System A)
# æ³¨æ„ï¼šä»¥ä¸‹è·¯å¾„ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
# è¯·æ ¹æ®å®é™…å®éªŒç»“æœä½ç½®ä¿®æ”¹
SYSTEM_A = {
    "label": "DOKE-RAG",
    "path": Path(
        "./evaluation/results/batch_experiment/cs0.2_tk40/stru_mech_result.json"
    ),
    "answer_key": "result",
}

# å¯¹æ‰‹ B: Light RAG (System B)
SYSTEM_B = {
    "label": "LightRAG",
    "path": Path(
        "./evaluation/results/batch_experiment/cs0.2_tk80/lightrag_result.json"
    ),
    "answer_key": "result",
}

# å¯¹æ‰‹ C: Graph RAG (System C)
SYSTEM_C = {
    "label": "GraphRAG",
    "path": Path(
        "./evaluation/results/GraphRAG_local_search_result.json"
    ),
    "answer_key": "response",
}

# è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨
METRICS = ["Comprehensiveness", "Diversity", "Empowerment", "Overall"]

# --- 2. è¾…åŠ©å‡½æ•° ---


def construct_prompt(query: str, answer1: str, answer2: str) -> str:
    """æ„å»ºè¯„ä¼°Promptï¼Œç¡®ä¿åŒ…å«4ä¸ªç»´åº¦çš„å®šä¹‰"""
    # æç¤º LLM å°½é‡é¿å…å¹³å±€
    return f"""
    Role: You are an expert evaluator tasked with systematically assessing two answers to the same question based on predefined criteria.
    Goal: Compare the two answers on the criteria below, providing a specific explanation for each. Finally, determine which answer is superior overall.
    Guiding Principle for Fairness: Your evaluation must weigh both the accuracy of the text and the effectiveness of any supporting materials. A high-quality answer excels in both. The ultimate measure is how effectively the entire answer conveys the necessary information and empowers the reader.
    Notice that differences in language should not affect the results of your judgment. **Your final "Winner" selection MUST be either "Answer 1" or "Answer 2". Avoid ties unless the answers are perfectly identical and equally good/bad.**
    i) Comprehensiveness: How thoroughly does the answer address all aspects of the question? For technical topics, this may include key formulas or diagrams. The focus should be on whether these components are **necessary for a complete answer and accurately presented**.
    ii) Diversity: How varied and rich is the answer in offering different perspectives and insights related to the question?
    iii) Empowerment: How effectively does the answer enable the reader to understand the topic? This is a critical measure of quality.
    - **Superiority of Good Visuals:** Under otherwise equal conditions, an answer that uses **correct, relevant, and well-explained** diagrams or formulas to clarify complex points **is superior** to a text-only answer. Such elements provide a more direct and intuitive path to understanding.
    - **Detriment of Bad Visuals:** Conversely, if an answer includes **irrelevant, incorrect, or confusing** supporting materials, it should be considered inferior to a clear and accurate text-only answer.
    iv) Overall: This dimension assesses the cumulative performance across the three preceding criteria to identify the best overall answer.
    Please strictly adhere to the following JSON format for your output. Do not include any text outside of the JSON structure.
    [Output Format]
    {{
      "Comprehensiveness": {{"Winner": "Answer 1 or Answer 2", "Explanation": "Your reasoning here"}},
      "Diversity": {{"Winner": "Answer 1 or Answer 2", "Explanation": "Your reasoning here"}},
      "Empowerment": {{"Winner": "Answer 1 or Answer 2", "Explanation": "Your reasoning here"}},
      "Overall": {{"Winner": "Answer 1 or Answer 2", "Explanation": "Your reasoning here"}}
    }}
    [Question]: {query}
    [Answers]
    Answer 1: {answer1}
    Answer 2: {answer2}
    """.strip()


async def call_deepseek(prompt: str, client: OpenAI) -> dict:
    """å¼‚æ­¥è°ƒç”¨ LLM"""

    def sync_call():
        try:
            completion = client.chat.completions.create(
                model="deepseek-r1",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                response_format={"type": "json_object"},
            )
            return {"content": completion.choices[0].message.content}
        except Exception as e:
            tqdm.write(f"  - API call failed: {e}")
            return {"content": "", "error": str(e)}

    return await asyncio.to_thread(sync_call)


def parse_evaluation_result(response_text: str) -> dict:
    """è§£æ JSON å“åº”"""
    if not response_text:
        return {}
    try:
        return json.loads(response_text)
    except Exception:
        try:
            # å°è¯•æå– markdown ä»£ç å—æˆ–å¯»æ‰¾é¦–å°¾æ‹¬å·
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1
            if json_start != -1 and json_end != -1:
                return json.loads(response_text[json_start:json_end])
            return {}
        except Exception:
            return {}


def load_all_answers(config: dict) -> dict:
    """åŠ è½½æŒ‡å®šè·¯å¾„çš„JSONæ–‡ä»¶ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º {query: answer} å­—å…¸ã€‚"""
    file_path = config["path"]
    label = config["label"]
    answer_key = config["answer_key"]

    if not file_path.exists():
        tqdm.write(f"é”™è¯¯: æœªæ‰¾åˆ°æ–‡ä»¶ '{label}' è·¯å¾„: {file_path}")
        return None

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        tqdm.write(f"é”™è¯¯: è¯»å–æˆ–è§£ææ–‡ä»¶ '{label}' å¤±è´¥ ({file_path}): {e}")
        return None

    answer_map = {}
    for item in data:
        query = item.get("query")
        answer = item.get(answer_key)
        if query and answer:
            answer_map[query.strip()] = answer
        elif query and not answer:
            tqdm.write(
                f"è­¦å‘Š: æ–‡ä»¶ '{label}' ä¸­ Query '{query[:30]}...' ç¼ºå°‘ç­”æ¡ˆå­—æ®µ '{answer_key}'ã€‚"
            )

    tqdm.write(f"æˆåŠŸä» '{label}' åŠ è½½äº† {len(answer_map)} ä¸ªé—®é¢˜-ç­”æ¡ˆå¯¹ã€‚")
    return answer_map


# --- 3. æ ¸å¿ƒé€»è¾‘: A vs B å’Œ A vs C ---


async def run_comparison(
    query_list: list,
    a_answers: dict,
    opponent_config: dict,
    client: OpenAI,
    run_id: int,
):
    """
    è¿è¡Œ A vs Opponent çš„å¯¹æ¯”ï¼Œå¹¶æ”¶é›†4ä¸ªç»´åº¦çš„ç»“æœ
    """

    opponent_answers = load_all_answers(opponent_config)
    if not opponent_answers:
        tqdm.write(f"è­¦å‘Š: æ— æ³•ä¸ºå¯¹æ‰‹ {opponent_config['label']} æ‰¾åˆ°ç­”æ¡ˆï¼Œè·³è¿‡å¯¹æ¯”ã€‚")
        return []

    results = []
    tasks = []
    semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)

    async def evaluate_pair(query):
        async with semaphore:
            query_key = query.strip()
            text_a = a_answers.get(query_key)
            text_op = opponent_answers.get(query_key)

            if not text_a or not text_op:
                return None

            # éšæœºäº¤æ¢ä½ç½®ä»¥é¿å…ä½ç½®åå·® (Position Bias)
            is_a_first = random.choice([True, False])
            if is_a_first:
                prompt = construct_prompt(query, text_a, text_op)
                slot_a = "Answer 1"
                slot_op = "Answer 2"
            else:
                prompt = construct_prompt(query, text_op, text_a)
                slot_a = "Answer 2"
                slot_op = "Answer 1"

            api_res = await call_deepseek(prompt, client)
            eval_json = parse_evaluation_result(api_res.get("content", ""))

            row = {
                "Experiment_Run": run_id,  # æ–°å¢å­—æ®µï¼šå®éªŒè½®æ¬¡
                "Query": query,
                "System_A": SYSTEM_A["label"],
                "System_Opponent": opponent_config["label"],
            }

            for metric in METRICS:
                winner_str = eval_json.get(metric, {}).get("Winner", "")

                # è®°å½• System A çš„èƒœè´ŸçŠ¶æ€
                if winner_str == slot_a:
                    result_label = SYSTEM_A["label"]  # A èµ¢
                elif winner_str == slot_op:
                    result_label = opponent_config["label"]  # Opponent èµ¢
                else:
                    result_label = "Tie/Unknown"  # å¹³å±€æˆ–æ— æ³•åˆ¤æ–­

                # è®°å½•è¯¦ç»†ç»“æœ
                row[f"{metric}_Winner"] = result_label
                row[f"{metric}_Reason"] = eval_json.get(metric, {}).get(
                    "Explanation", ""
                )

            return row

    # ç­›é€‰åªåœ¨ A å’Œ Opponent ä¸­éƒ½å­˜åœ¨çš„ Query è¿›è¡Œ PK
    valid_queries = [
        q
        for q in query_list
        if q.strip() in a_answers and q.strip() in opponent_answers
    ]
    tqdm.write(
        f"  - ç¬¬ {run_id} è½®ï¼š{SYSTEM_A['label']} vs {opponent_config['label']} æœ‰ {len(valid_queries)} ä¸ªå…±åŒé—®é¢˜ç”¨äº PKã€‚"
    )

    tasks = [evaluate_pair(query) for query in valid_queries]

    completed_matches = await tqdm_asyncio.gather(
        *tasks,
        desc=f"  - ç¬¬ {run_id} è½®è¯„ä¼° {SYSTEM_A['label']} vs {opponent_config['label']} è¿›åº¦",
    )

    # è¿‡æ»¤æ‰æ— æ•ˆç»“æœ
    results = [r for r in completed_matches if r is not None]
    return results


def generate_summary_report(all_results: list) -> pd.DataFrame:
    """
    ä»è¯¦ç»†ç»“æœä¸­ç”Ÿæˆæœ€ç»ˆçš„èƒœç‡ç»Ÿè®¡æŠ¥å‘Šï¼Œå¹³å±€å°†ä»æ€»åœºæ¬¡ä¸­æ’é™¤ã€‚
    """
    df = pd.DataFrame(all_results)
    summary_rows = []

    # æŒ‰ç…§å¯¹æ‰‹åˆ†ç»„
    for opponent, subset_op in df.groupby("System_Opponent"):
        # å®šä¹‰è¡Œæ ‡ç­¾
        row_comp = {"Metric": "Comprehensiveness"}
        row_emp = {"Metric": "Empowerment"}
        row_div = {"Metric": "Diversity"}
        row_over = {"Metric": "Overall"}

        # ç»Ÿè®¡å››ä¸ªæŒ‡æ ‡çš„å¹³å‡èƒœç‡
        for metric in METRICS:
            # ç­›é€‰å‡ºæœ‰æ˜ç¡®èƒœè´Ÿçš„åœºæ¬¡ (æ’é™¤å¹³å±€)
            subset_metric = subset_op[
                subset_op[f"{metric}_Winner"] != "Tie/Unknown"
            ].copy()
            total_valid = len(subset_metric)  # è®¡å…¥ PK æ€»åœºæ¬¡çš„åœºæ¬¡

            # ç»Ÿè®¡ A èµ¢ã€å¯¹æ‰‹èµ¢çš„æ•°é‡
            wins_a = len(
                subset_metric[subset_metric[f"{metric}_Winner"] == SYSTEM_A["label"]]
            )

            if total_valid > 0:
                # è®¡ç®— A çš„èƒœç‡å’Œå¯¹æ‰‹çš„èƒœç‡ (ä¿è¯ä¸¤è€…ç›¸åŠ ä¸º 100%)
                win_rate_a = wins_a / total_valid
                win_rate_op = 1.0 - win_rate_a

                # ç»Ÿè®¡æ‰€æœ‰è½®æ¬¡ä¸­ï¼Œå¹³å±€çš„å¹³å‡æ¯”ä¾‹ï¼ˆä»…ä½œå‚è€ƒä¿¡æ¯ï¼‰
                total_runs = len(subset_op)
                ties = total_runs - total_valid
                avg_tie_rate = ties / total_runs
            else:
                win_rate_a = 0.0
                win_rate_op = 0.0
                avg_tie_rate = 0.0

            # å¡«å……ç»Ÿè®¡æ•°æ®åˆ°å¯¹åº”çš„è¡Œå­—å…¸
            if metric == "Comprehensiveness":
                row = row_comp
            elif metric == "Empowerment":
                row = row_emp
            elif metric == "Diversity":
                row = row_div
            else:  # Overall
                row = row_over

            # ä»¥ç™¾åˆ†æ¯”æ˜¾ç¤ºï¼Œå¹¶åŠ ä¸Šæ’é™¤å¹³å±€åçš„æœ‰æ•ˆåœºæ¬¡ä¿¡æ¯
            row[SYSTEM_A["label"]] = f"{win_rate_a:.1%}"
            row[opponent] = f"{win_rate_op:.1%}"
            row["Total Valid Matches"] = total_valid
            row["Avg Tie Rate"] = f"{avg_tie_rate:.1%}"

        # æ•´ç†æˆç±»ä¼¼å›¾ç‰‡çš„æ ¼å¼ï¼Œæ¯ç»„å¯¹æ¯”åŒ…å« 4 è¡Œ
        summary_rows.append({"Opponent_Group": opponent, **row_comp})
        summary_rows.append({"Opponent_Group": opponent, **row_emp})
        summary_rows.append({"Opponent_Group": opponent, **row_div})
        summary_rows.append({"Opponent_Group": opponent, **row_over})

    # é‡æ–°æ„é€ è¡¨æ ¼ï¼Œä½¿å…¶æ›´åƒå›¾ç‰‡æ ·å¼
    final_pivot_data = []
    for opponent in pd.unique([r["Opponent_Group"] for r in summary_rows]):
        final_pivot_data.append(
            {
                "Metric": "",
                SYSTEM_A["label"]: "",
                opponent: "",
                "Total Valid Matches": "",
                "Avg Tie Rate": "",
            }
        )  # ç©ºè¡Œç”¨äºåˆ†éš”
        final_pivot_data.append(
            {
                "Metric": f"--- {opponent} ---",
                SYSTEM_A["label"]: "---",
                opponent: "---",
                "Total Valid Matches": "---",
                "Avg Tie Rate": "---",
            }
        )

        # ç­›é€‰å‡ºå½“å‰å¯¹æ‰‹çš„ 4 ä¸ªæŒ‡æ ‡è¡Œ
        subset = [r for r in summary_rows if r["Opponent_Group"] == opponent]
        for row in subset:
            final_pivot_data.append(
                {
                    "Metric": row["Metric"],
                    SYSTEM_A["label"]: row[SYSTEM_A["label"]],
                    opponent: row[opponent],
                    "Total Valid Matches": row["Total Valid Matches"],
                    "Avg Tie Rate": row["Avg Tie Rate"],
                }
            )

    # ç¡®ä¿åˆ—åé¡ºåº
    columns_order = [
        "Metric",
        SYSTEM_A["label"],
        opponent,
        "Total Valid Matches",
        "Avg Tie Rate",
    ]
    return pd.DataFrame(final_pivot_data)


# --- 4. ä¸»ç¨‹åº ---


async def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    load_dotenv(dotenv_path=ENV_FILE_PATH)
    api_key = os.getenv("ALIYUN_API_KEY")
    if not api_key:
        print(
            f"é”™è¯¯: æ‰¾ä¸åˆ° ALIYUN_API_KEYã€‚è¯·æ£€æŸ¥ .env æ–‡ä»¶ (è·¯å¾„: {ENV_FILE_PATH}) æˆ–ç¯å¢ƒå˜é‡è®¾ç½®ã€‚"
        )
        return

    client = OpenAI(
        api_key=api_key,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )

    output_dir = Path(OUTPUT_DIRECTORY)
    output_dir.mkdir(exist_ok=True, parents=True)

    # 1. åŠ è½½æ‰€æœ‰ç³»ç»Ÿçš„ç­”æ¡ˆ
    tqdm.write("\n--- 1. åŠ è½½æ‰€æœ‰ç³»ç»Ÿç­”æ¡ˆ (åªåŠ è½½ä¸€æ¬¡) ---")
    a_answers = load_all_answers(SYSTEM_A)
    if not a_answers:
        return

    unique_queries = sorted(a_answers.keys())
    tqdm.write(f"ä»¥ {SYSTEM_A['label']} ä¸ºåŸºå‡†ï¼Œå…±æ‰¾åˆ° {len(unique_queries)} ä¸ªé—®é¢˜ã€‚")

    # 2. è¿è¡Œ N è½® PK å¯¹æ¯”
    tqdm.write(f"\n--- 2. è¿è¡Œ {NUM_EXPERIMENT_RUNS} è½® PK å¯¹æ¯”: A vs B & A vs C ---")
    opponents = [SYSTEM_B, SYSTEM_C]
    all_results = []

    # å¤–å±‚å¾ªç¯ï¼šé‡å¤ N æ¬¡å®éªŒ
    for run_id in tqdm(range(1, NUM_EXPERIMENT_RUNS + 1), desc="æ‰€æœ‰å®éªŒè½®æ¬¡"):
        for opponent in opponents:
            tqdm.write(
                f"\n{'=' * 20} ç¬¬ {run_id}/{NUM_EXPERIMENT_RUNS} è½®å¯¹æ¯”: {SYSTEM_A['label']} vs {opponent['label']} {'=' * 20}"
            )

            # run_comparison å†…éƒ¨ä¼šéšæœºäº¤æ¢ä½ç½®
            comparison_results = await run_comparison(
                unique_queries, a_answers, opponent, client, run_id
            )
            all_results.extend(comparison_results)

    # 3. ç»“æœå¤„ç†ä¸ä¿å­˜
    if not all_results:
        print("\nè¯„ä¼°æœªèƒ½ç”Ÿæˆä»»ä½•æœ‰æ•ˆç»“æœã€‚")
        return

    # ä¿å­˜è¯¦ç»† CSV æŠ¥å‘Š (åŒ…å«æ‰€æœ‰ 5 è½®çš„ç»“æœ)
    df_detail = pd.DataFrame(all_results)
    detail_csv = output_dir / "Detailed_Comparison_5Runs.csv"
    df_detail.to_csv(detail_csv, index=False, encoding="utf-8-sig")
    print(f"\nğŸ‰ è¯¦ç»†å¯¹æ¯”ç»“æœ ({NUM_EXPERIMENT_RUNS} è½®) å·²ä¿å­˜è‡³: {detail_csv}")

    # ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š (å¹³å‡ç»“æœ)
    df_summary_pivot = generate_summary_report(all_results)
    summary_csv = output_dir / "Summary_WinRates_Average_Report.csv"
    df_summary_pivot.to_csv(summary_csv, index=False, encoding="utf-8-sig")

    print("\n" + "="*70)
    print(f"  ğŸ“Š æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š ({NUM_EXPERIMENT_RUNS} è½®å¹³å‡èƒœç‡ï¼Œå·²æ’é™¤å¹³å±€)")
    print("="*70)

    # è§£å†³æ‰“å°é”™è¯¯: ä½¿ç”¨ df.to_string() æ›¿æ¢ df.to_markdown()
    print(df_summary_pivot.to_string(index=False))

if __name__ == "__main__":
    asyncio.run(main())